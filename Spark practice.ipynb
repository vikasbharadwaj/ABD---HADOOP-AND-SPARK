{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test RDD\").getOrCreate()\n",
    "# IN ABOVE COMMAND WE HAVE CREATED AN APPLICATION NAMED TEST RDD..IF THIS APPLICATION NAME EXISTS IT WILL GET IT OR IT WILL CREATE A ANEW APPLICATION.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "['hi', 'hello', 'How u doing', 'bakwas']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Using Parallelize method---- data which is acquired from runtime application but not stored anywhere; so u store in rdd using parallized method\n",
    "\n",
    "# it takes two parameters: Data and no of splits\n",
    "rdd_par = spark.sparkContext.parallelize([\"hi\",\"hello\",\"How u doing\",\"bakwas\"])\n",
    "print(type(rdd_par))\n",
    "print(rdd_par.collect())         #collect() is an example for action.....it is used to see the content of the file\n",
    "print(rdd_par.count())           #count() gives no of records..count()[0] will give first record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How u doing']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating rdd using transformations\n",
    "# filter() is an api under rdd. it takes how to filter\n",
    "\n",
    "rdd_trans = rdd_par.filter(lambda word:word.startswith('H'))\n",
    "rdd_trans.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "['The Job output data between each step has to be stored in the distributed file system before the next step can begin.', 'Hence, this approach tends to be slow due to replication & disk storage.', 'Also, Hadoop solutions typically include clusters that are hard to set up and manage.', 'It also requires the integration of several tools for different big data use cases.']\n"
     ]
    }
   ],
   "source": [
    "# Creating rdd using datasource\n",
    "\n",
    "rdd_ds = spark.sparkContext.textFile('shreyapractice.txt')\n",
    "print(rdd_ds.count())\n",
    "print(rdd_ds.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The map operation takes a Function, which is called for each value in the input stream and produces one result value, which is sent to the output stream.\n",
    "\n",
    "#The flatMap operation takes a function that conceptually wants to consume one value and produce an arbitrary number of values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# To count total no of words in the given text file.\n",
    "rdd_ds.flatMap(lambda word:word.split(' ')).collect()      # it give all the words which are present in record\n",
    "rdd_ds.flatMap(lambda word:word.split(' ')).count()        # count of total number of words in the record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 1),\n",
       " ('Job', 1),\n",
       " ('output', 1),\n",
       " ('step', 2),\n",
       " ('stored', 1),\n",
       " ('in', 1),\n",
       " ('before', 1),\n",
       " ('begin.', 1),\n",
       " ('this', 1),\n",
       " ('approach', 1),\n",
       " ('tends', 1),\n",
       " ('due', 1),\n",
       " ('disk', 1),\n",
       " ('typically', 1),\n",
       " ('include', 1),\n",
       " ('clusters', 1),\n",
       " ('are', 1),\n",
       " ('set', 1),\n",
       " ('manage.', 1),\n",
       " ('It', 1),\n",
       " ('integration', 1),\n",
       " ('of', 1),\n",
       " ('several', 1),\n",
       " ('tools', 1),\n",
       " ('different', 1),\n",
       " ('use', 1),\n",
       " ('data', 2),\n",
       " ('between', 1),\n",
       " ('each', 1),\n",
       " ('has', 1),\n",
       " ('to', 4),\n",
       " ('be', 2),\n",
       " ('the', 3),\n",
       " ('distributed', 1),\n",
       " ('file', 1),\n",
       " ('system', 1),\n",
       " ('next', 1),\n",
       " ('can', 1),\n",
       " ('Hence,', 1),\n",
       " ('slow', 1),\n",
       " ('replication', 1),\n",
       " ('&', 1),\n",
       " ('storage.', 1),\n",
       " ('Also,', 1),\n",
       " ('Hadoop', 1),\n",
       " ('solutions', 1),\n",
       " ('that', 1),\n",
       " ('hard', 1),\n",
       " ('up', 1),\n",
       " ('and', 1),\n",
       " ('also', 1),\n",
       " ('requires', 1),\n",
       " ('for', 1),\n",
       " ('big', 1),\n",
       " ('cases.', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find out how many words appear each time in a record\n",
    "# 1. find list of individual words using split()\n",
    "# 2. for each word create a tuple\n",
    "word_rdd = rdd_ds.flatMap(lambda word:word.split(' '))\n",
    "\n",
    "freq_words = word_rdd.map(lambda word: (word, 1))\n",
    "#freq_words.collect()\n",
    "\n",
    "# In spark we have reduce by key function. input required for reduce by key function is key value pair/ tuple.\n",
    "# in reduce by key function it has builtin accumulator . initially 0..keeps adding to give the count for each word in reord\n",
    "\n",
    "freq_words.reduceByKey(lambda a,b : a+b).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|       _1| _2|\n",
      "+---------+---+\n",
      "|     Euro| 90|\n",
      "|    Pound|100|\n",
      "|     Yuan| 11|\n",
      "|      Yen|  2|\n",
      "|Us Dollar| 74|\n",
      "|  k dinar|242|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['currency', 'value']\n",
    "inputdata = [('Euro', 90), ('Pound', 100), ('Yuan', 11), ('Yen', 2), ('Us Dollar', 74), ('k dinar', 242)]\n",
    "\n",
    "# Creating Dataframe using RDD\n",
    "rdd = spark.sparkContext.parallelize(inputdata)   # Creating rdd\n",
    "rddDF = rdd.toDF()                                # To create a df\n",
    "rddDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "| Currency| _2|\n",
      "+---------+---+\n",
      "|     Euro| 90|\n",
      "|    Pound|100|\n",
      "|     Yuan| 11|\n",
      "|      Yen|  2|\n",
      "|Us Dollar| 74|\n",
      "|  k dinar|242|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to give column names\n",
    "\n",
    "df = rddDF.withColumnRenamed('_1', \"Currency\")\n",
    "#df = rddDF.withColumnRenamed('_2', \"Value\")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "| currency|value|\n",
      "+---------+-----+\n",
      "|     Euro|   90|\n",
      "|    Pound|  100|\n",
      "|     Yuan|   11|\n",
      "|      Yen|    2|\n",
      "|Us Dollar|   74|\n",
      "|  k dinar|  242|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instead of renaming the column names everytime, we will use column names\n",
    "\n",
    "df = spark.createDataFrame(rdd).toDF(*columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "| currency|value|\n",
      "+---------+-----+\n",
      "|     Euro|   90|\n",
      "|    Pound|  100|\n",
      "|     Yuan|   11|\n",
      "|      Yen|    2|\n",
      "|Us Dollar|   74|\n",
      "|  k dinar|  242|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating df without rdd\n",
    "df = spark.createDataFrame(data = inputdata, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the dataframe in csv file\n",
    "\n",
    "df.write.format('csv').save('C:/Users/HP/Desktop/SPARK/test')\n",
    "\n",
    "# created a folder named test.... and solit the dataframe in to two parts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we don't want partition\n",
    "\n",
    "df.repartition(1).write.format('csv').save('C:/Users/HP/Desktop/SPARK/test1', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the dataframe in txt file \n",
    "\n",
    "df.rdd.map(lambda x: x[0] + \",\" + str(x[1])).repartition(1).saveAsTextFile('C:/Users/HP/Desktop/SPARK/text1')\n",
    "\n",
    "\n",
    "# rdd does not have write.format...so saveastextfile\n",
    "# Str()....convert integer to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|       value|\n",
      "+------------+\n",
      "|     Euro,90|\n",
      "|   Pound,100|\n",
      "|     Yuan,11|\n",
      "|       Yen,2|\n",
      "|Us Dollar,74|\n",
      "| k dinar,242|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To read a txt file\n",
    "\n",
    "dftxt = spark.read.text(\"C:/Users/HP/Desktop/SPARK/text1\")\n",
    "dftxt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "| currency|value|\n",
      "+---------+-----+\n",
      "|     Euro|   90|\n",
      "|    Pound|  100|\n",
      "|     Yuan|   11|\n",
      "|      Yen|    2|\n",
      "|Us Dollar|   74|\n",
      "|  k dinar|  242|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to read a csv file\n",
    "\n",
    "#dfcsv = spark.read.csv(\"C:/Users/HP/Desktop/SPARK/test1\")\n",
    "\n",
    "dfcsv = spark.read.csv(\"C:/Users/HP/Desktop/SPARK/test1\", header = True)\n",
    "dfcsv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To read table from mysql\n",
    "#dfmysql = spark.read.format('jdbc')\\\n",
    "#    .option(\"url\", \"jdbc:mysql://ipaddress or url/retail_db\")\\\n",
    "#    .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "#    .option(\"dbtable\",\"orders\")\\\n",
    "#    .option(\"user\", \"sois\")\\\n",
    "#    .option(\"password\", \"manipal\")\\\n",
    "#    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating dataframe from datasource\n",
    "\n",
    "df = spark.read.format('csv')\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load('2015-summary.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Destination  |\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "|United States|\n",
      "|Egypt        |\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|United States    |\n",
      "|United States    |\n",
      "|United States    |\n",
      "|Egypt            |\n",
      "|United States    |\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# performing operations on data\n",
    "\n",
    "from pyspark.sql.functions import col, expr, column, udf, date_sub, date_add, col, datediff, regexp_extract\n",
    "from pyspark.sql.types import StringType, IntegerType \n",
    "\n",
    "\n",
    "# To find data of 1 column\n",
    "\n",
    "# various ways of selecting columns\n",
    "\n",
    "#df.select(col('DEST_COUNTRY_NAME')).show(5, False)\n",
    "\n",
    "#df.select(column('DEST_COUNTRY_NAME')).show(5, False)\n",
    "\n",
    "#df.select('DEST_COUNTRY_NAME').show(5, False)\n",
    "\n",
    "#df.select('DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME').show(5, False)\n",
    "\n",
    "df.select(expr('DEST_COUNTRY_NAME AS Destination')) .show(5, False)\n",
    "df.select('DEST_COUNTRY_NAME').show(5, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|United States    |Romania            |15   |false        |\n",
      "|United States    |Croatia            |1    |false        |\n",
      "|United States    |Ireland            |344  |false        |\n",
      "|Egypt            |United States      |15   |false        |\n",
      "|United States    |India              |62   |false        |\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To add column in data frame\n",
    "df.withColumn('withinCountry', expr('ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME')).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|Frequency|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|United States    |Romania            |15   |Normal   |\n",
      "|United States    |Croatia            |1    |Min      |\n",
      "|United States    |Ireland            |344  |Busy     |\n",
      "|Egypt            |United States      |15   |Normal   |\n",
      "|United States    |India              |62   |More     |\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding column by user defined function\n",
    "\n",
    "def computeGroup(count):\n",
    "    if count < 2:\n",
    "        return 'Min'\n",
    "    elif count < 20:\n",
    "        return 'Normal'\n",
    "    elif count < 100:\n",
    "        return 'More'\n",
    "    else:\n",
    "        return 'Busy'\n",
    "\n",
    "group_udf = udf(computeGroup, StringType())\n",
    "df.withColumn('Frequency', group_udf(col('count'))).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|                type|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|\n",
      "|     14|        Nixon (1995)|               Drama|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
      "|     16|       Casino (1995)|         Crime|Drama|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|\n",
      "|     18|   Four Rooms (1995)|              Comedy|\n",
      "|     19|Ace Ventura: When...|              Comedy|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviedf = spark.read.format('csv')\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferschema', 'true')\\\n",
    "    .load('movie.csv')\n",
    "moviedf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|title                             |\n",
      "+----------------------------------+\n",
      "|Toy Story (1995)                  |\n",
      "|Jumanji (1995)                    |\n",
      "|Grumpier Old Men (1995)           |\n",
      "|Waiting to Exhale (1995)          |\n",
      "|Father of the Bride Part II (1995)|\n",
      "+----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviedf.select('title').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+----+\n",
      "|movieId|               title|                type|Year|\n",
      "+-------+--------------------+--------------------+----+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|1995|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|1995|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|1995|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|1995|\n",
      "|      5|Father of the Bri...|              Comedy|1995|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|1995|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|1995|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|1995|\n",
      "|      9| Sudden Death (1995)|              Action|1995|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|1995|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|1995|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|1995|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|1995|\n",
      "|     14|        Nixon (1995)|               Drama|1995|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|1995|\n",
      "|     16|       Casino (1995)|         Crime|Drama|1995|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|1995|\n",
      "|     18|   Four Rooms (1995)|              Comedy|1995|\n",
      "|     19|Ace Ventura: When...|              Comedy|1995|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|1995|\n",
      "+-------+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movieYear = moviedf.withColumn('Year',regexp_extract(col('title'), r\"(\\d\\d\\d\\d)\", 1).cast(IntegerType()))\n",
    "movieYear.show()\n",
    "\n",
    "cleanedMovie = movieYear.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+----+------+\n",
      "|movieId|               title|                type|Year|Decade|\n",
      "+-------+--------------------+--------------------+----+------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|1995|  1990|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|1995|  1990|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|1995|  1990|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|1995|  1990|\n",
      "|      5|Father of the Bri...|              Comedy|1995|  1990|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|1995|  1990|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|1995|  1990|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|1995|  1990|\n",
      "|      9| Sudden Death (1995)|              Action|1995|  1990|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|1995|  1990|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|1995|  1990|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|1995|  1990|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|1995|  1990|\n",
      "|     14|        Nixon (1995)|               Drama|1995|  1990|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|1995|  1990|\n",
      "|     16|       Casino (1995)|         Crime|Drama|1995|  1990|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|1995|  1990|\n",
      "|     18|   Four Rooms (1995)|              Comedy|1995|  1990|\n",
      "|     19|Ace Ventura: When...|              Comedy|1995|  1990|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|1995|  1990|\n",
      "+-------+--------------------+--------------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calDecade(years):\n",
    "    return (years - years%10)\n",
    "\n",
    "decadeudf = udf(calDecade, IntegerType())\n",
    "movieDecade = cleanedMovie.withColumn('Decade', decadeudf(col('Year')).cast(IntegerType()))\n",
    "movieDecade.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+----+------+\n",
      "|movieId|               title|                type|Year|Decade|\n",
      "+-------+--------------------+--------------------+----+------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|1995|  1990|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|1995|  1990|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|1995|  1990|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|1995|  1990|\n",
      "|      5|Father of the Bri...|              Comedy|1995|  1990|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|1995|  1990|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|1995|  1990|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|1995|  1990|\n",
      "|      9| Sudden Death (1995)|              Action|1995|  1990|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|1995|  1990|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|1995|  1990|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|1995|  1990|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|1995|  1990|\n",
      "|     14|        Nixon (1995)|               Drama|1995|  1990|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|1995|  1990|\n",
      "|     16|       Casino (1995)|         Crime|Drama|1995|  1990|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|1995|  1990|\n",
      "|     18|   Four Rooms (1995)|              Comedy|1995|  1990|\n",
      "|     19|Ace Ventura: When...|              Comedy|1995|  1990|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|1995|  1990|\n",
      "+-------+--------------------+--------------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movieDecade.filter('Decade == 1990').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date & Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp, date_sub, date_add, col, datediff, to_date,to_timestamp, lit, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+\n",
      "|id |today     |now                    |\n",
      "+---+----------+-----------------------+\n",
      "|0  |2021-01-28|2021-01-28 06:06:21.031|\n",
      "|1  |2021-01-28|2021-01-28 06:06:21.031|\n",
      "|2  |2021-01-28|2021-01-28 06:06:21.031|\n",
      "|3  |2021-01-28|2021-01-28 06:06:21.031|\n",
      "|4  |2021-01-28|2021-01-28 06:06:21.031|\n",
      "+---+----------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "dateDF = spark.range(10)\\\n",
    "    .withColumn('today', current_date())\\\n",
    "    .withColumn('now', current_timestamp())\n",
    "dateDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
